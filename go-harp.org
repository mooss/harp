#+title: GoHarp, Harp implementation in Go
#+author: FÃ©lix Jamet

#+property: header-args :noweb yes :results output
#+property: header-args:go+ :main no

* Prelude

** References

Peter Norvig's Lispy[fn:: See https://norvig.com/lispy.html.] is the main inspiration for this attempt at writing an interpreter, because I find it very high level, informative, pleasant to read and easy to follow.
I have borrowed some terminology and examples from it.


** Inclusion

LitLib's =include.pl= provides the inclusion and dependency resolution functionalities that bind together the code blocks scattered throughout this document.

#+name: include
#+begin_src bash :var code_blocks="" :wrap src go
./litlib/include.pl go-harp.org ":noweb $code_blocks"
#+end_src


** Go utils :noexport:

This section provides callable code blocks providing various Go related functionalities.
They are only used for development purposes.

*** Ensure Go is available in the =$PATH=

Using the official go installation instruction, =go= and =gofmt= are located under =/usr/local/go/bin=, which must therefore be added to the =$PATH=.

#+name: PATHgo
#+begin_src bash
PATH="$PATH:/usr/local/go/bin"
#+end_src

*** Go doc

This code block can be used to query Go's documentation.

#+name: go-doc
#+begin_src bash :var package_and_args=""
<<PATHgo>>
go doc $package_and_args 2>&1 || echo "No doc for \`$package_and_args\`."
#+end_src

*** =gofmt= diff

Shows the diff of a code block with the output of =gofmt=, this allows to easily see what is expected by =gofmt=.

#+name: gofmt-diff
#+begin_src bash :var codeblock="" args="" :wrap src diff
tangled=$(./litlib/include.pl go-harp.org ":noweb $codeblock :exit-with-error")
if [ $? -ne 0 ]; then
    echo "$tangled"
    exit 0
fi
<<PATHgo>>
gofmt -d <(echo -e "package dummy\n"; echo "$tangled") $args 2>&1 || echo "-gofmt failed."
#+end_src


When taking the ill-formated code block below as reference,
#+name: gofmt-diff-example
#+begin_src go
func badlyFormated(source string) int {
	result := (len(source)*2)+1
	return  result
}
#+end_src

=gofmt-diff= can highlight the formatting errors:
#+Call: gofmt-diff("gofmt-diff-example")

#+RESULTS:
#+begin_src diff
diff -u /dev/fd/63.orig /dev/fd/63
--- /dev/fd/63.orig	2022-01-16 18:05:28.527962295 +0100
+++ /dev/fd/63	2022-01-16 18:05:28.527962295 +0100
@@ -1,6 +1,6 @@
 package dummy
 
 func badlyFormated(source string) int {
-	result := (len(source)*2)+1
-	return  result
+	result := (len(source) * 2) + 1
+	return result
 }
#+end_src


* Tokenization

Tokenization's role is to break a Harp source code into its tokens, i.e. substrings representing the individual components of the program.

** =Token=

Tokens are stored in the =Token= type:
#+begin_src go :noweb-ref Token
type Token struct {
	string
	category TokenCategory
}
#+end_src

*** =TokenCategory=

Tokenization is kept rather simple, with only a handful of different categories of tokens allowed:
#+begin_src go :noweb-ref Token
type TokenCategory int

const (
	ErrorToken  TokenCategory = iota // Avoid accidental construction of empty tokens.
	BalexpToken                      // Balanced expression delimiters, like parens.
	StringToken                      // "a double quoted string"
	PlainToken                       // Any other valid token.
	EndOfSource                      // No more tokens (not an error).
)
#+end_src

The =TokenCategory= defined above illustrate the syntactic simplicity of Harp, with only 3 categories of valid tokens:
 - Balexp (for balanced expression) tokens are pairs of runes delimiting an expression, one opening it and the other closing it.
 - Strings are enclosed within a pair of non-escaped double quotes and can contain whitespace.
 - Plain tokens are any valid token that is neither a balexp nor a string, they will later be semantically separated into symbols, numbers and errors during the parsing phase.

In the semantically nonsensical but syntactically correct Harp program below, all the valid token categories are present.
#+begin_src scheme :eval never
(category (string "a \"double quoted\" string")
          (plain 23 any-other-valid-token))
#+end_src

Regrouped by category, we have:
 - =(=, =)= as balanced expressions delimiters,
 - =category=, =string=, =plain=, =23=, =any-other-valid-token= as plain tokens and
 - ="a \"double quoted\" string"= as a string token[fn::Note the spaces and the escaped double quotes.].

*** Plain token predicate

This predicate determines which runes can go in a plain token.
I choose to make it quite restrictive so that interesting runes like =[]{}#= can be used in a future implementation for other purposes.
At worst it's easy to enlarge the definition by altering the predicate.

#+name: isPlainTokenRune
#+begin_src go
// isPlainTokenRune test for characters that can go anywhere in a plain token.
func isPlainTokenRune(r rune) bool {
	return (r >= 42 && r <= 57) || (r >= 65 && r <= 90) || (r >= 97 && r <= 122)
}
#+end_src

Usage and list of runes conforming to the predicate:
#+begin_src go :results output :exports both :wrap example
package main

import(
	"fmt"
)

<<include("isPlainTokenRune")>>

func main() {
	for r := rune(0); r < 127; r++ {
		if isPlainTokenRune(r) {
			fmt.Print(string(r))
		}
	}
	fmt.Println()
}
#+end_src

#+RESULTS:
#+begin_example
,*+,-./0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz
#+end_example

My best attempt at a simple sentence describing this set of runes is the following: "ASCII letters, digits or one of *+,-./".

*** Rune verification

The rune verification functions centralize two aspects:
 1. Validation of the pertinent predicate.
 2. Production of the appropriate error message.
Doing those two things at the same place makes them less likely to drift apart from one another.

#+name: verifyPlainTokenStart
#+begin_src go
// verifyPlainTokenStart returns an error if its argument is not a valid first rune for a plain token.
func verifyPlainTokenStart(start rune) error {
	if isPlainTokenRune(start) || start == '\'' { // Quoting is allowed.
		return nil
	}
	template := "invalid start of token `%v`, expected a plain token rune (ASCII letters, digits or one of *+,-./) or a single quote"
	return fmt.Errorf(template, string(start))
}
#+end_src
#+depends:verifyPlainTokenStart :noweb isPlainTokenRune :go unicode errors fmt

#+name: verifyPlainTokenContent
#+begin_src go
// verifyPlainTokenContent returns an error if its argument is not a valid rune for a plain token.
func verifyPlainTokenContent(content rune) error {
	if isPlainTokenRune(content) {
		return nil
	}
	template := "invalid token content `%v`, expected a plain token rune (ASCII letters, digits or one of *+,-./)"
	return fmt.Errorf(template, string(content))
}
#+end_src
#+depends:verifyPlainTokenContent :noweb isPlainTokenRune :go unicode errors fmt


** =tokenizer=

The =tokenizer= type represents a step in the tokenization process, with =start= pointing at the first rune of the token being processed and =end= pointing after its last rune.
I call =start= and =end= pointers because they are indeed pointing at a slice within =source=, even though they are not technically pointers.
What is between =start= and =end= is called the current token.

#+begin_src go :noweb-ref tokenizer
type tokenizer struct {
	source     string
	start, end int
}
#+end_src
#+depends:tokenizer :noweb Token verifyPlainTokenStart verifyPlainTokenContent :go errors unicode unicode/utf8

=tokenizer= is built around the invariant src_go[]{start == end},
special care has been taken to maintain it in all its exported functions.
It means that, as long as only the exported functions are used, =tokenizer= is never in an intermediate state where it is in the middle of producing a token, which could result in incorrect tokens.

The next subsections will present in a bottom-up manner the functions build around =tokenizer=, starting with =tokenizer= construction, then iteration primitives, then multiple-rune iteration and finally to the end goal of =tokenizer=, token production.

*** Construction

The first step is of course to start at the very first character of the source.

#+begin_src go :noweb-ref tokenizer
func NewTokenizer(source string) tokenizer {
	return tokenizer{source, 0, 0}
}
#+end_src

The invariant is maintained by =NewTokenizer= because, well, src_go[]{0 == 0}.

*** Iteration primitives

Iteration is not done via a ranged for loop, but rather via a test of whether the iteration is =Over=, and a method moving to the =nextRune=.
This is done because the tokenization process is not strictly linear, hence the need to =backtrack=.
It also allows to perform iteration within functions that can be nested as needed, which is not possible with a for loop.

#+begin_src go :noweb-ref tokenizer
// Over returns true if the tokenization has reached the end of the source code.
func (tok tokenizer) Over() bool {
	return tok.end >= len(tok.source)
}

// nextRune advances to the next rune and returns it along with its size.
func (tok *tokenizer) nextRune() (rune, int) {
	r, size := utf8.DecodeRuneInString(tok.source[tok.end:])
	tok.end += size
	return r, size
}

// backtrack rewinds the end pointer, thus reversing the actions of nextRune.
func (tok *tokenizer) backtrack(s int) {
	tok.end -= s
}
#+end_src

The invariant is maintained by =Over= because its =tokenizer= is not modified.

*** Multiple-rune iteration

The =skip*= and =leap*= methods move forward in the source code several runes at a time, the difference between the two is in what they are moving.
=skip*= move both the start and the end to the same point whereas =leap*= moves only the end.
Thus after a =skip*=, the current token is the empty string, whereas after a =leap*=, the current token is longer than or equal to what it was before.

Outside of strings, sequences of whitespace are used to separates tokens, the exact composition of a whitespace sequence does not change its meaning, hence this function to skip them:
#+begin_src go :noweb-ref tokenizer
// skipWhitespace makes the tokenizer point to the next non-whitespace character.
func (tok *tokenizer) skipWhitespace() {
	defer func() { tok.start = tok.end }()
	for leap, r := range tok.source[tok.end:] {
		if !unicode.IsSpace(r) {
			tok.end += leap
			return
		}
	}
	tok.end = len(tok.source)
}
#+end_src

The =leapToEndOf*= functions are defining two crucial things:
 1. Which runes can terminate a token category.
 2. Which runes are valid in a token category.

For plain tokens, termination is a space or a closing paren and =verifyPlainTokenContent= handles character validity:
#+begin_src go :noweb-ref tokenizer
func (tok *tokenizer) leapToEndOfPlainToken() error {
	for !tok.Over() {
		r, s := tok.nextRune() // (
		if unicode.IsSpace(r) || r == ')' {
			tok.backtrack(s) // r is not a part of plain token.
			break
		}
		if err := verifyPlainTokenContent(r); err != nil {
			return err
		}
	}
	return nil
}
#+end_src

For strings, termination is a non-escaped double quote and all non-empty characters are valid:
#+begin_src go :noweb-ref tokenizer
func (tok *tokenizer) leapToEndOfString() error {
	escaped := false
	for !tok.Over() {
		r, _ := tok.nextRune()
		switch {
		case r == '"' && !escaped: // "
			return nil
		case r == '\\':
			escaped = !escaped // Covers both escaping anything and a backslash being escaped.
		default:
			if escaped {
				escaped = false
			}
		}
	}
	return errors.New("unterminated string literal")
}
#+end_src

*** Token production

This minor utility makes it more readable and less error-prone to get the current token:
#+begin_src go :noweb-ref tokenizer
func (tok tokenizer) currentToken() string {
	return tok.source[tok.start:tok.end]
}
#+end_src

=NextToken= fullfils the purpose of =tokenizer=, it produces the next token and move the =start= and =end= pointers after it.
#+begin_src go :noweb-ref tokenizer
func (tok *tokenizer) NextToken() (Token, error) {
	defer func() { tok.start = tok.end }()

	tok.skipWhitespace()
	if tok.Over() {
		return Token{"", EndOfSource}, nil
	}

	r, _ := tok.nextRune()
	switch r {
	case '(', ')':
		return Token{tok.currentToken(), BalexpToken}, nil

	case '"': // "
		if err := tok.leapToEndOfString(); err != nil {
			return Token{tok.currentToken(), ErrorToken}, err
		}
		return Token{tok.currentToken(), StringToken}, nil

	default:
		if err := verifyPlainTokenStart(r); err != nil {
			return Token{tok.currentToken(), ErrorToken}, err
		}
		if err := tok.leapToEndOfPlainToken(); err != nil {
			return Token{tok.currentToken(), ErrorToken}, err
		}
		return Token{tok.currentToken(), PlainToken}, nil
	}
}
#+end_src

The invariant is maintained by =NextToken= thanks to the =defer= statement at the top.


** =Tokenize= function

This exported function assembles all the tokens into an easy to consume array, thus making =tokenizer= an implementation detail.
At first I thought about using a channel like a Python-style generator but I eventually realised that this was not as practical as generators are in Python and, for this use case, not better than just cramming all tokens into an array.

Note that by construction, only valid token types are returned, =ErrorToken= and =EndOfSource= cannot be emitted by =Tokenize=.

#+name: Tokenize
#+begin_src go
func Tokenize(source string) ([]Token, error) {
	tokz := NewTokenizer(source)
	result := []Token{}
	for !tokz.Over() {
		token, err := tokz.NextToken()
		if err == nil && token.category == ErrorToken {
			err = errors.New("unknown tokenization error")
		}
		if err != nil {
			return nil, fmt.Errorf("Tokenization error: %v.", err)
		}
		if token.category == EndOfSource {
			break
		}
		result = append(result, token)
	}
	return result, nil
}
#+end_src
#+depends:Tokenize :noweb Token tokenizer :go fmt errors

Usage:
#+begin_src go :results output :wrap src default :exports both
package main

<<include("Tokenize :go strings fmt")>>

func decomposeTokens(source string) {
	tokens, err := Tokenize(source)
	fmt.Println()
	if err != nil {
		fmt.Printf("Failed to tokenize `%s` ; %s\n", source, err.Error())
		return
	}
	buffer := []string{}
	for _, l := range tokens {
		buffer = append(buffer, fmt.Sprintf("`%v`", l.string))
	}
	fmt.Println(strings.Join(buffer, ", "))
}

func main() {
	example := "(category (string \"a \\\"double quoted\\\" string\")\n          (plain 23 any-other-valid-token))"
	fmt.Println("Example source is:")
	fmt.Println(example)
	decomposeTokens(example)
	decomposeTokens(fmt.Sprintf("[%s]", example))
	decomposeTokens(fmt.Sprintf("%s#", example))
}
#+end_src

#+RESULTS:
#+begin_src default
Example source is:
(category (string "a \"double quoted\" string")
          (plain 23 any-other-valid-token))

`(`, `category`, `(`, `string`, `"a \"double quoted\" string"`, `)`, `(`, `plain`, `23`, `any-other-valid-token`, `)`, `)`

Failed to tokenize `[(category (string "a \"double quoted\" string")
          (plain 23 any-other-valid-token))]` ; Tokenization error: invalid start of token `[`, expected a plain token rune (ASCII letters, digits or one of *+,-./) or a single quote.

Failed to tokenize `(category (string "a \"double quoted\" string")
          (plain 23 any-other-valid-token))#` ; Tokenization error: invalid start of token `#`, expected a plain token rune (ASCII letters, digits or one of *+,-./) or a single quote.
#+end_src


* Parsing

The goal of parsing is to uncover the structure of a program, transforming it from a flat representation (a sequence of runes or a sequence of tokens) to a hierarchical data structure (an abstract syntax tree, aka AST) representative how it should be executed.

Its implementation is done in a style similar to that of tokenization, with a =parser= type and its associated functions.

** AST, elements, H-arrays and expressions

Lisp code, and by extension Harp code, has the interesting property of being written in a manner closely ressembling its AST, with parens being used to group elements at the same level.

The leaf types of Harp's abstract syntax tree are called *elements* and are represented as structs with one =val= field:
#+begin_src go :noweb-ref harp-elements
type Int struct{ val int64 }
type Float struct{ val float64 }
type String struct{ val string }
type Symbol struct{ val string }
#+end_src

Since the premise of Harp is to use arrays instead of lists, a Harp AST is represented with slices, Go's name for dynamic arrays.

Those elements can be accumulated inside a H-array (=Harray=):

#+name: Harray
#+begin_src go
type Harray struct {
	elements []Expression
}
#+end_src

Note that Harray does not store values of type =Element= but rather of type =Expression=, something that can be either an element or an H-array.
This way, a H-array can truly encode the hierarchical nature of an AST.

#+depends:harp-expression :noweb harp-elements Harray

The =Expression= type is an interface that will be defined when pertinent, because specifying the interface will depend on what we want to do with it and parsing is not concerned with what to do with expressions, only with how to represent and build them.


** Parser primitives

A =parser= is represented as an array of not-yet-parsed tokens:
#+begin_src go :noweb-ref parser
type parser struct {
	tokens []Token
}
#+end_src
#+depends:parser :noweb harp-elements :go fmt unicode strconv

This array shrinks every time a token is consumed:
#+begin_src go :noweb-ref parser
func (p *parser) consumeToken() Token {
	result := p.tokens[0]
	p.tokens = p.tokens[1:]
	return result
}
#+end_src

Consuming a token should only be done after this predicate has been verified:
#+begin_src go :noweb-ref parser
func (p parser) tokensRemain() bool {
	return len(p.tokens) > 0
}
#+end_src


** Parse one expression

All the actual parsing logic is contained in =makeExpression=, able to construct an =Expression=, which as a reminder is either an element or a H-array.
The most obvious cases are handled directly in the code below.
The less obvious cases will be treated in the next subsections.


#+begin_src go :noweb-ref parser
func (p *parser) makeExpression(token Token) (Expression, error) {
	switch token.category {
	case ErrorToken:
		return nil, fmt.Errorf("unexpected error token `%s`", token.string)

	case EndOfSource:
		return nil, fmt.Errorf("unexpected end of source token `%s`", token.string)

	case StringToken:
		// We trust the tokenization phase to produce correct strings.
		// Interpretation of backslashes remains to be done.
		return String{token.string[1 : len(token.string)-1]}, nil

	case PlainToken:
		<<Parse plain token>>

	case BalexpToken:
		<<Parse balanced expression>>
	}

	return nil, fmt.Errorf("unhandled token `%s` of category `%d`", token.string, token.category)
}
#+end_src


** Parse plain token

Only numbers (=Float=, =Int=) and symbols (=Symbol=) remains to be handled.

In Harp's eyes, a number literal always starts with a digit and is sometimes preceded by a minus sign.
If the token is not a number, all possibilities have been exhausted and since the tokenization process is assumed to be correct, the token can only be a symbol.

#+name: Parse plain token
#+begin_src go
r := rune(token.string[0])
switch {
case unicode.IsDigit(r) || (r == '-' && len(token.string) >= 2 && unicode.IsDigit(rune(token.string[1]))):
	<<Parse number>>
default:
	return Symbol{token.string}, nil
}
#+end_src

*** Parse number

In Harp, float literals must have a dot, e.g. =8.= and =8.0= are float literals representing 8.
This property is used to make the difference between ints and floats:
#+name: Parse number
#+begin_src go
assumeFloat := false
for _, r := range token.string[1:] {
	if r == '.' {
		assumeFloat = true
		break
	}
}

if assumeFloat {
	value, err := strconv.ParseFloat(token.string, 64)
	if err != nil {
		return nil, fmt.Errorf("float parse error (%v)", err)
	}
	return Float{value}, nil
}

base := 10 // Only base 10 numbers for now.
value, err := strconv.ParseInt(token.string, base, 64)
if err != nil {
	return nil, fmt.Errorf("int parse error (%v)", err)
}
return Int{value}, nil
#+end_src

*** Parse balanced expression

The only balanced expression supported is a H-array, specified between pairs of matching parens.
#+name: Parse balanced expression
#+begin_src go
if len(token.string) != 1 {
	return nil, fmt.Errorf("invalid balanced token `%s` (only ASCII runes are valid)", token.string)
}
switch token.string[0] {
case '(':
	<<Construct H-array>>
case ')':
	<<Handle unexpected closing token>>
}
return nil, fmt.Errorf("unhandled balanced token `%s`", token.string)
#+end_src

H-arrays contain expressions produced by recursive calls to =makeExpression=.
Their hierarchical nature comes from the fact that those expressions can themselves be H-arrays.

#+name: Construct H-array
#+begin_src go
harray := Harray{}
for p.tokensRemain() {
	nestedToken := p.consumeToken() // (
	if len(nestedToken.string) == 1 && nestedToken.string[0] == ')' {
		return harray, nil
	}
	nestedExpr, err := p.makeExpression(nestedToken)
	if err != nil {
		return nil, err
	}
	harray.elements = append(harray.elements, nestedExpr)
}
return nil, fmt.Errorf("ran out of tokens without closing `%s`", token.string)
#+end_src


Closing tokens should only be found inside the for loop above, finding them outside means that they are not associated with an open token, therefore rendering the expression invalid:
#+name: Handle unexpected closing token
#+begin_src go
return nil, fmt.Errorf("unbalanced expression, found unmatched `%s`", token.string)
#+end_src


** Parse the next expression

Where =makeExpression= can return any type of expression, =NextHarray= can only return an H-array
#+name: NextHarray
#+begin_src go :noweb-ref parser
func (p *parser) NextHarray() (Harray, error) {
	token := p.consumeToken()
	expr, err := p.makeExpression(token)
	if err != nil {
		return Harray{}, err
	}

	res, valid := expr.(Harray)
	if valid {
		return res, nil
	}

	return Harray{}, fmt.Errorf("expression is not an H-array, but a %T", res)
}
#+end_src

The caller is responsible for ensuring that at least one token remains.


** Parse functions

This final section defines parse functions constructing an AST from a sequence of tokens and from a raw string.
A simple AST printer is also defined as a way to illustrate the AST constructed by those functions.

*** =ParseTokens=

=ParseTokens= tries to spend all its token into a valid H-array.

#+name: ParseTokens
#+begin_src go
func ParseTokens(tokens []Token) (Harray, error) {
	p := parser{tokens}
	if !p.tokensRemain() {
		return Harray{}, fmt.Errorf("no token to parse")
	}
	result, err := p.NextHarray()
	if err != nil {
		return Harray{}, err
	}
	if !p.tokensRemain() {
		return result, nil
	}

	result = Harray{[]Expression{Symbol{"begin"}, result}}
	for p.tokensRemain() {
		nested, err := p.NextHarray()
		if err != nil {
			return Harray{}, err
		}
		result.elements = append(result.elements, nested)
	}
	return result, nil
}
#+end_src
#+depends:ParseTokens :noweb Token harp-expression parser :go fmt

When multiple H-arrays are present, they are regrouped under a =begin= statement, thus the following code is valid:
#+begin_src scheme
(define x 4)
(define y 8.)
#+end_src
It will be implicitely transformed by =ParseTokens= into:
#+begin_src scheme
(begin (define x 4)
       (define y 8.))
#+end_src

*** =PrintAST=

The important thing =PrintAST= tries to do is to properly align the elements.
In the implementaion below, this is controlled by the =inline= and =depth= parameters, although there are probably better ways to handle this.

#+name: PrintAST
#+begin_src go
func recPrintAST(ast Harray, depth int, inline bool) {
	baseIndentation := " "
	indentation := func() string { return strings.Repeat(baseIndentation, depth) }

	if inline {
		depth++
	} else {
		fmt.Print(indentation())
	}
	fmt.Print("(")
	defer fmt.Print(")")
	if len(ast.elements) == 0 {
		return
	}

	printExpression := func(el Expression, inline bool) {
		nestedAst, isHarray := el.(Harray)
		if isHarray {
			recPrintAST(nestedAst, depth, inline)
			return
		}
		if !inline {
			fmt.Print(indentation())
		}
		elType := reflect.TypeOf(el)
		fmt.Printf("%v%v", elType.Name(), el)
	}

	el := ast.elements[0]
	printExpression(el, true)
	depth++ // Account for the opening paren.
	for _, el := range ast.elements[1:] {
		fmt.Println()
		printExpression(el, false)
	}
}

func PrintAST(ast Harray) {
	recPrintAST(ast, 0, false)
	fmt.Println()
}
#+end_src
#+depends:PrintAST :noweb harp-expression :go strings fmt reflect

*** =ParseTokens= illustration

#+begin_src go :results output :exports both :wrap example
package main

<<include(":noweb Token ParseTokens PrintAST :go fmt")>>

type Expression interface{} // Must be defined.

func main() {
	tokens := []Token{
		{"(", BalexpToken}, {"define", PlainToken}, {"x", PlainToken}, {"4", PlainToken}, {")", BalexpToken},
		{"(", BalexpToken}, {"define", PlainToken}, {"y", PlainToken}, {"8.", PlainToken}, {")", BalexpToken},
	}
	fmt.Println("Raw tokens:")
	fmt.Println(tokens)

	ast, err := ParseTokens(tokens)
	if err != nil {
		fmt.Println(err)
		return
	}
	fmt.Println("\nRaw AST:")
	fmt.Println(ast)
	fmt.Println("\nFormatted AST:")
	PrintAST(ast)
}
#+end_src

#+RESULTS:
#+begin_example
Raw tokens:
[{( 1} {define 3} {x 3} {4 3} {) 1} {( 1} {define 3} {y 3} {8. 3} {) 1}]

Raw AST:
{[{begin} {[{define} {x} {4}]} {[{define} {y} {8}]}]}

Formatted AST:
(Symbol{begin}
 (Symbol{define}
  Symbol{x}
  Int{4})
 (Symbol{define}
  Symbol{y}
  Float{8}))
#+end_example

The result is basically a less readable version of the source program with type annotations.
We can also witness the wrapping of each H-array into a =begin= expression.

*** =ParseString=

There is not much to do but to rely on =Tokenize= and =ParseTokens=:
#+name: ParseString
#+begin_src go
func ParseString(source string) (Harray, error) {
	tokens, err := Tokenize(source)
	if err != nil {
		return Harray{}, err
	}

	return ParseTokens(tokens)
}
#+end_src
#+depends:ParseString :noweb Tokenize ParseTokens

Usage:
#+begin_src go :results output :exports both :wrap example
package main

<<include(":noweb ParseString PrintAST :go fmt")>>

type Expression interface{} // Must be defined.

func example(program string) {
	fmt.Println("Program:")
	fmt.Println(program)
	fmt.Println()
	ast, err := ParseString(program)
	if err != nil {
		fmt.Println("Parsing error:", err.Error() + ".")
		return
	}

	fmt.Println("Raw AST:")
	fmt.Println(ast)

	fmt.Println("\nFormatted AST:")
	PrintAST(ast)
	fmt.Println()
}

func main() {
	example("(define x 4) (define y 8.)")
	example("(begin (define r 10) (* pi (* r r)))")
	example("(begin (define sentence \"AurÃ« entuluva\"))")
}
#+end_src

#+RESULTS:
#+begin_example
Program:
(define x 4) (define y 8.)

Raw AST:
{[{begin} {[{define} {x} {4}]} {[{define} {y} {8}]}]}

Formatted AST:
(Symbol{begin}
 (Symbol{define}
  Symbol{x}
  Int{4})
 (Symbol{define}
  Symbol{y}
  Float{8}))

Program:
(begin (define r 10) (* pi (* r r)))

Raw AST:
{[{begin} {[{define} {r} {10}]} {[{*} {pi} {[{*} {r} {r}]}]}]}

Formatted AST:
(Symbol{begin}
 (Symbol{define}
  Symbol{r}
  Int{10})
 (Symbol{*}
  Symbol{pi}
  (Symbol{*}
   Symbol{r}
   Symbol{r})))

Program:
(begin (define sentence "AurÃ« entuluva"))

Raw AST:
{[{begin} {[{define} {sentence} {AurÃ« entuluva}]}]}

Formatted AST:
(Symbol{begin}
 (Symbol{define}
  Symbol{sentence}
  String{AurÃ« entuluva}))
#+end_example
